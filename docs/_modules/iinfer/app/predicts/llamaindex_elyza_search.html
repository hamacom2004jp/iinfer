<!DOCTYPE html>
<html class="writer-html5" lang="jp" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>iinfer.app.predicts.llamaindex_elyza_search &mdash; iinfer 2024/05/18 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=3fafac33" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=1aa140b2"></script>
        <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />


</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            iinfer
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/overview.html">iinferの概要</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/install.html">インストール</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/gui.html">GUIモード</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/web.html">Webモード</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/command.html">コマンドリファレンス</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/models.html">動作確認済みモデル</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/detection.html">物体検知</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/segmentation.html">領域検知</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/face.html">顔認識</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/pipeline.html">パイプライン</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/injections.html">インジェクション</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/customize.html">カスタマイズ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/developer.html">開発者向け情報</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/errors.html">よくあるエラー</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../docs/license.html">ライセンス</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">iinfer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">iinfer.app.predicts.llamaindex_elyza_search</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for iinfer.app.predicts.llamaindex_elyza_search</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">iinfer.app</span> <span class="kn">import</span> <span class="n">common</span><span class="p">,</span> <span class="n">predict</span>
<span class="kn">from</span> <span class="nn">iinfer.app.commons</span> <span class="kn">import</span> <span class="n">convert</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">logging</span>


<span class="n">SITE</span> <span class="o">=</span> <span class="s1">&#39;https://huggingface.co/mmnga/ELYZA-japanese-Llama-2-7b-fast-instruct-gguf/tree/main&#39;</span>
<span class="n">IMAGE_WIDTH</span> <span class="o">=</span> <span class="mi">640</span>
<span class="n">IMAGE_HEIGHT</span> <span class="o">=</span> <span class="mi">640</span>
<span class="n">REQUIREd_MODEL_CONF</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">REQUIREd_MODEL_WEIGHT</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="LlamaIndex_ELYZA_Search">
<a class="viewcode-back" href="../../../../resources/iinfer.app.predicts.html#iinfer.app.predicts.llamaindex_elyza_search.LlamaIndex_ELYZA_Search">[docs]</a>
<span class="k">class</span> <span class="nc">LlamaIndex_ELYZA_Search</span><span class="p">(</span><span class="n">predict</span><span class="o">.</span><span class="n">TorchPredict</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logger</span><span class="p">:</span><span class="n">logging</span><span class="o">.</span><span class="n">Logger</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">logger</span><span class="p">)</span>

<div class="viewcode-block" id="LlamaIndex_ELYZA_Search.post_deploy">
<a class="viewcode-back" href="../../../../resources/iinfer.app.predicts.html#iinfer.app.predicts.llamaindex_elyza_search.LlamaIndex_ELYZA_Search.post_deploy">[docs]</a>
    <span class="k">def</span> <span class="nf">post_deploy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deploy_dir</span><span class="p">:</span><span class="n">Path</span><span class="p">,</span> <span class="n">conf</span><span class="p">:</span><span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        デプロイ後の処理を行う関数です。</span>
<span class="sd">        deployコマンド実行時に呼び出されます。</span>
<span class="sd">        この関数内でデプロイ後の処理を実装してください。</span>
<span class="sd">        </span>
<span class="sd">        Args:</span>
<span class="sd">            deploy_dir (Path): デプロイディレクトリのパス</span>
<span class="sd">            conf (dict): デプロイ設定</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="LlamaIndex_ELYZA_Search.create_session">
<a class="viewcode-back" href="../../../../resources/iinfer.app.predicts.html#iinfer.app.predicts.llamaindex_elyza_search.LlamaIndex_ELYZA_Search.create_session">[docs]</a>
    <span class="k">def</span> <span class="nf">create_session</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deploy_dir</span><span class="p">:</span><span class="n">Path</span><span class="p">,</span> <span class="n">model_path</span><span class="p">:</span><span class="n">Path</span><span class="p">,</span> <span class="n">model_conf_path</span><span class="p">:</span><span class="n">Path</span><span class="p">,</span> <span class="n">model_provider</span><span class="p">:</span><span class="nb">str</span><span class="p">,</span> <span class="n">gpu_id</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        推論セッションを作成する関数です。</span>
<span class="sd">        startコマンド実行時に呼び出されます。</span>
<span class="sd">        この関数内でAIモデルのロードを行い、推論準備を完了するようにしてください。</span>
<span class="sd">        戻り値の推論セッションの型は問いません。</span>

<span class="sd">        Args:</span>
<span class="sd">            deploy_dir (Path): デプロイディレクトリのパス</span>
<span class="sd">            model_path (Path): モデルファイルのパス</span>
<span class="sd">            model_conf_path (Path): モデル設定ファイルのパス</span>
<span class="sd">            gpu_id (int, optional): GPU ID. Defaults to None.</span>

<span class="sd">        Returns:</span>
<span class="sd">            推論セッション</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">conf</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">loadopt</span><span class="p">(</span><span class="n">deploy_dir</span> <span class="o">/</span> <span class="s1">&#39;conf.json&#39;</span><span class="p">)</span>
        <span class="n">opt</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">loadopt</span><span class="p">(</span><span class="n">conf</span><span class="p">[</span><span class="s1">&#39;model_conf_file&#39;</span><span class="p">])</span> <span class="k">if</span> <span class="n">conf</span><span class="p">[</span><span class="s1">&#39;model_conf_file&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">dict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">temperature</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">getopt</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="s1">&#39;temperature&#39;</span><span class="p">,</span> <span class="n">preval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">withset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_ctx</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">getopt</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="s1">&#39;n_ctx&#39;</span><span class="p">,</span> <span class="n">preval</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">withset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_gpu_layers</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">getopt</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="s1">&#39;n_gpu_layers&#39;</span><span class="p">,</span> <span class="n">preval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">withset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># elyza/ELYZA-japanese-Llama-2-7b-fast-instruct</span>
        <span class="n">model_name</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">getopt</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="s1">&#39;model_name&#39;</span><span class="p">,</span> <span class="n">preval</span><span class="o">=</span><span class="n">model_path</span><span class="p">,</span> <span class="n">withset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">embedd_model_name</span> <span class="o">=</span> <span class="n">common</span><span class="o">.</span><span class="n">getopt</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="s1">&#39;embedd_model_name&#39;</span><span class="p">,</span> <span class="n">preval</span><span class="o">=</span><span class="s1">&#39;intfloat/multilingual-e5-base&#39;</span><span class="p">,</span> <span class="n">withset</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">doc_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">common</span><span class="o">.</span><span class="n">getopt</span><span class="p">(</span><span class="n">opt</span><span class="p">,</span> <span class="s1">&#39;doc_path&#39;</span><span class="p">,</span> <span class="n">preval</span><span class="o">=</span><span class="n">deploy_dir</span><span class="o">/</span><span class="s1">&#39;docs&#39;</span><span class="p">,</span> <span class="n">withset</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">doc_path</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
            <span class="n">doc_path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span> <span class="n">SimpleDirectoryReader</span><span class="p">,</span> <span class="n">Settings</span>
        <span class="kn">from</span> <span class="nn">llama_index.core.ingestion</span> <span class="kn">import</span> <span class="n">IngestionPipeline</span>
        <span class="kn">from</span> <span class="nn">llama_index.core.node_parser</span> <span class="kn">import</span> <span class="n">SentenceSplitter</span>
        <span class="kn">from</span> <span class="nn">llama_index.embeddings.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbedding</span>
        <span class="kn">from</span> <span class="nn">llama_index.llms.huggingface</span> <span class="kn">import</span> <span class="n">HuggingFaceLLM</span>
        <span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>
        <span class="kn">import</span> <span class="nn">torch</span>

        <span class="n">documents</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="n">input_dir</span><span class="o">=</span><span class="n">doc_path</span><span class="p">,</span> <span class="n">exclude_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
        <span class="n">Settings</span><span class="o">.</span><span class="n">text_splitter</span> <span class="o">=</span> <span class="n">SentenceSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
        <span class="n">Settings</span><span class="o">.</span><span class="n">transformations</span> <span class="o">=</span> <span class="p">[</span><span class="n">Settings</span><span class="o">.</span><span class="n">text_splitter</span><span class="p">]</span>
        <span class="n">Settings</span><span class="o">.</span><span class="n">embed_model</span> <span class="o">=</span> <span class="n">HuggingFaceEmbedding</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">embedd_model_name</span><span class="p">,</span> <span class="n">cache_folder</span><span class="o">=</span><span class="n">deploy_dir</span><span class="p">)</span>
        <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
            <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>
            <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
            <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">Settings</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">HuggingFaceLLM</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">tokenizer_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
                                      <span class="n">context_window</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="c1"># maximum input size to the LLM</span>
                                      <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="c1"># number of tokens reserved for text generation.</span>
                                      <span class="n">model_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">cache_dir</span><span class="o">=</span><span class="n">deploy_dir</span><span class="p">),</span>
                                      <span class="n">generate_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.95</span><span class="p">))</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">embed_model</span><span class="o">=</span><span class="n">Settings</span><span class="o">.</span><span class="n">embed_model</span><span class="p">)</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        import llama_index</span>
<span class="sd">        import langchain</span>
<span class="sd">        llm = langchain.llms.LlamaCpp(model_path=conf[&#39;model_file&#39;], temperature=self.temperature, n_ctx=self.n_ctx, n_gpu_layers=self.n_gpu_layers)</span>
<span class="sd">        llm_predictor = llama_index.llm_predictor.LLMPredictor(llm=llm)</span>
<span class="sd">        embed_model = llama_index.embeddings.LangchainEmbedding(</span>
<span class="sd">                        langchain.embeddings.huggingface.HuggingFaceEmbeddings(model_name=self.model_name))</span>
<span class="sd">        service_context = llama_index.ServiceContext.from_defaults(llm_predictor=llm_predictor, embed_model=embed_model)</span>
<span class="sd">        # ドキュメントのインデックス化</span>
<span class="sd">        documents = llama_index.SimpleDirectoryReader(&#39;./inputs&#39;).load_data()</span>
<span class="sd">        model = llama_index.GPTVectorStoreIndex.from_documents(documents, service_context=service_context)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">index</span></div>


<div class="viewcode-block" id="LlamaIndex_ELYZA_Search.predict">
<a class="viewcode-back" href="../../../../resources/iinfer.app.predicts.html#iinfer.app.predicts.llamaindex_elyza_search.LlamaIndex_ELYZA_Search.predict">[docs]</a>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">img_width</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">img_height</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">input_data</span><span class="p">:</span><span class="n">Union</span><span class="p">[</span><span class="n">Image</span><span class="o">.</span><span class="n">Image</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">labels</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">colors</span><span class="p">:</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">nodraw</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        予測を行う関数です。</span>
<span class="sd">        predictコマンドやcaptureコマンド実行時に呼び出されます。</span>
<span class="sd">        引数のinput_dataが画像の場合RGBですので、戻り値の出力画像もRGBにしてください。</span>
<span class="sd">        戻り値の推論結果のdictは、通常推論結果項目ごとに値(list)を設定します。</span>

<span class="sd">        Args:</span>
<span class="sd">            model: 推論セッション</span>
<span class="sd">            img_width (int): モデルのINPUTサイズ（input_dataが画像の場合は、画像の幅）</span>
<span class="sd">            img_height (int): モデルのINPUTサイズ（input_dataが画像の場合は、画像の高さ）</span>
<span class="sd">            input_data (Image | str): 推論するデータ（画像の場合RGB配列であること）</span>
<span class="sd">            labels (List[str], optional): クラスラベルのリスト. Defaults to None.</span>
<span class="sd">            colors (List[Tuple[int]], optional): ボックスの色のリスト. Defaults to None.</span>
<span class="sd">            nodraw (bool, optional): 描画フラグ. Defaults to False.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[Dict[str, Any], Image]: 予測結果と出力画像(RGB)のタプル</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">template</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">            &lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; こんにちは、私はELYZAです。私は日本語の質問に答えることができます。</span>
<span class="s2">            あなたは誠実で優秀な日本人のアシスタントです。</span>
<span class="s2">            以下の「コンテキスト情報」と「制約条件」を元に質問に回答してください。</span>

<span class="s2">            # コンテキスト情報</span>

<span class="s2">            ---------------------</span>
<span class="s2">            </span><span class="si">{context_str}</span>
<span class="s2">            ---------------------</span>

<span class="s2">            # 制約条件</span>

<span class="s2">            - コンテキスト情報はマークダウン形式で書かれています。</span>
<span class="s2">            - 「コンテキスト情報に」のような書き方を絶対に回答に含めないでください。</span>
<span class="s2">            - コンテキスト情報に無い情報は絶対に回答に含めないでください。</span>
<span class="s2">            - コンテキスト情報の内容を丸投げするのではなく、絶対にきちんとした文章にして回答してください。</span>
<span class="s2">            - 質問の答えを知らない場合は、誤った情報を共有しないでください。</span>
<span class="s2">            &lt;&lt;/SYS&gt;&gt;</span>

<span class="s2">            </span><span class="si">{query_str}</span><span class="s2"> [/INST]</span>
<span class="s2">            &quot;&quot;&quot;</span>

        <span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span> <span class="n">Settings</span>
        <span class="n">index</span><span class="p">:</span><span class="n">VectorStoreIndex</span> <span class="o">=</span> <span class="n">model</span>
        <span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span><span class="c1">#text_qa_template=llama_index.prompts.prompts.QuestionAnswerPrompt(template))</span>
        <span class="n">res_msg</span> <span class="o">=</span> <span class="n">query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">input_data</span><span class="p">,</span> <span class="n">responce</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">res_msg</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()),</span> <span class="kc">None</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Copyright (c) 2023-2024 hamacom2004jp All Rights Reserved..</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>